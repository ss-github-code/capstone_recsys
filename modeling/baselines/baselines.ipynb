{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import ast\n",
    "from itertools import chain\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Electronics_5.json.gz --output Electronics_5.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_Electronics.json.gz --output electronics_metadata.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting JSON to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield json.loads(l.strip())\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('./raw/Electronics_5.json.gz')\n",
    "df_meta = getDF('./raw/electronics_metadata.json.gz')\n",
    "df.shape, df_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning reviews DataFrame\n",
    "df = df.drop(columns=['image','reviewTime'])\n",
    "df['reviewTime'] = pd.to_datetime(df['unixReviewTime'], unit='s')\n",
    "df = df.drop(columns=['reviewText','summary','reviewerName'])\n",
    "df.drop('style', axis=1).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning meta DataFrame\n",
    "df_meta = df_meta[['asin','main_cat','title']]\n",
    "df_meta = df_meta.drop_duplicates()\n",
    "df_meta = df_meta[~df_meta['main_cat'].str.contains('img src')]\n",
    "df_meta['main_cat'] = df_meta['main_cat'].replace(r'^\\s*$', 'others', regex=True)\n",
    "df_meta.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Reviews and Meta Datasets\n",
    "\n",
    "data = df.merge(df_meta,how='inner', left_on='asin', right_on='asin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sequential Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = data[['reviewerID','asin','main_cat','reviewTime']]\n",
    "seq = seq.sort_values('reviewTime')\n",
    "\n",
    "# Calculating length of sequences\n",
    "seq = seq.groupby('reviewerID').agg(list).reset_index()\n",
    "seq['length'] = seq.apply(lambda row: len(row['asin']), axis=1)\n",
    "\n",
    "print('Mean length of sequences per user: {}'.format(seq['length'].mean()))\n",
    "print('Median length of sequences per user: {}'.format(seq['length'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.to_csv('./seq_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Datsets for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0003214FKMKJE0PCW3D</td>\n",
       "      <td>[B000P1O73A, B008562SXS, B00BFO14W8, B004XVN1V...</td>\n",
       "      <td>['Computers', 'Home Audio &amp; Theater', 'Cell Ph...</td>\n",
       "      <td>[Timestamp('2014-10-29 00:00:00'), Timestamp('...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A00101847G3FJTWYGNQA</td>\n",
       "      <td>[B00AWRUIY4, B00C7NSIO8, B005F778JO, B006T9B6R...</td>\n",
       "      <td>['Computers', 'Computers', 'All Electronics', ...</td>\n",
       "      <td>[Timestamp('2013-09-19 00:00:00'), Timestamp('...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A00222906VX8GH7X6J6B</td>\n",
       "      <td>[B00DI89IQS, B00EPIWY2U, B00JFR8UQ0, B00J8DL78...</td>\n",
       "      <td>['Home Audio &amp; Theater', 'Computers', 'Compute...</td>\n",
       "      <td>[Timestamp('2015-01-22 00:00:00'), Timestamp('...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A0072193KFP6LUHKEXLT</td>\n",
       "      <td>[B00008N6Y8, B00AF56QA8, B000I97G0U, B000EPNB5...</td>\n",
       "      <td>['All Electronics', 'Home Audio &amp; Theater', 'C...</td>\n",
       "      <td>[Timestamp('2013-10-24 00:00:00'), Timestamp('...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A007917716EGEEP4D8LB</td>\n",
       "      <td>[B00HSGR8PE, B0063X4BNK, B00SJVCT88, B000WJTEN...</td>\n",
       "      <td>['Computers', 'Computers', 'Computers', 'All E...</td>\n",
       "      <td>[Timestamp('2015-10-15 00:00:00'), Timestamp('...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524812</th>\n",
       "      <td>AZZXJAE2DILET</td>\n",
       "      <td>[B00DKFF386, B00DU45VWU, B0013FRNKG, B00EAM87E...</td>\n",
       "      <td>['Computers', 'Computers', 'Computers', 'Compu...</td>\n",
       "      <td>[Timestamp('2014-08-13 00:00:00'), Timestamp('...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524813</th>\n",
       "      <td>AZZY4W8E5AX2K</td>\n",
       "      <td>[B0092U4140, B0046S54GC, B008HCX6S6, B00HZZGY8...</td>\n",
       "      <td>['Amazon Devices', 'Home Audio &amp; Theater', 'Ca...</td>\n",
       "      <td>[Timestamp('2013-10-21 00:00:00'), Timestamp('...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524814</th>\n",
       "      <td>AZZYJH0XNZ896</td>\n",
       "      <td>[B006WHPQSQ, B0099TX7O4, B008X8NK0I, B00E3R0S1...</td>\n",
       "      <td>['Home Audio &amp; Theater', 'Computers', 'Compute...</td>\n",
       "      <td>[Timestamp('2014-06-14 00:00:00'), Timestamp('...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524816</th>\n",
       "      <td>AZZYW4YOE1B6E</td>\n",
       "      <td>[B001AK0496, B002PHM0G8, B004Z0S7K6, B009FWEKO...</td>\n",
       "      <td>['Home Audio &amp; Theater', 'Computers', 'All Ele...</td>\n",
       "      <td>[Timestamp('2009-01-28 00:00:00'), Timestamp('...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524818</th>\n",
       "      <td>AZZZU3P1UQZ0C</td>\n",
       "      <td>[B0007NWL70, B01DMHPT3U, B007PUMCWC, B00ESW9SZ...</td>\n",
       "      <td>['Home Audio &amp;amp; Theater', 'All Electronics'...</td>\n",
       "      <td>[Timestamp('2014-12-29 00:00:00'), Timestamp('...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307257 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  reviewerID  \\\n",
       "1       A0003214FKMKJE0PCW3D   \n",
       "3       A00101847G3FJTWYGNQA   \n",
       "4       A00222906VX8GH7X6J6B   \n",
       "8       A0072193KFP6LUHKEXLT   \n",
       "9       A007917716EGEEP4D8LB   \n",
       "...                      ...   \n",
       "524812         AZZXJAE2DILET   \n",
       "524813         AZZY4W8E5AX2K   \n",
       "524814         AZZYJH0XNZ896   \n",
       "524816         AZZYW4YOE1B6E   \n",
       "524818         AZZZU3P1UQZ0C   \n",
       "\n",
       "                                                     asin  \\\n",
       "1       [B000P1O73A, B008562SXS, B00BFO14W8, B004XVN1V...   \n",
       "3       [B00AWRUIY4, B00C7NSIO8, B005F778JO, B006T9B6R...   \n",
       "4       [B00DI89IQS, B00EPIWY2U, B00JFR8UQ0, B00J8DL78...   \n",
       "8       [B00008N6Y8, B00AF56QA8, B000I97G0U, B000EPNB5...   \n",
       "9       [B00HSGR8PE, B0063X4BNK, B00SJVCT88, B000WJTEN...   \n",
       "...                                                   ...   \n",
       "524812  [B00DKFF386, B00DU45VWU, B0013FRNKG, B00EAM87E...   \n",
       "524813  [B0092U4140, B0046S54GC, B008HCX6S6, B00HZZGY8...   \n",
       "524814  [B006WHPQSQ, B0099TX7O4, B008X8NK0I, B00E3R0S1...   \n",
       "524816  [B001AK0496, B002PHM0G8, B004Z0S7K6, B009FWEKO...   \n",
       "524818  [B0007NWL70, B01DMHPT3U, B007PUMCWC, B00ESW9SZ...   \n",
       "\n",
       "                                                 main_cat  \\\n",
       "1       ['Computers', 'Home Audio & Theater', 'Cell Ph...   \n",
       "3       ['Computers', 'Computers', 'All Electronics', ...   \n",
       "4       ['Home Audio & Theater', 'Computers', 'Compute...   \n",
       "8       ['All Electronics', 'Home Audio & Theater', 'C...   \n",
       "9       ['Computers', 'Computers', 'Computers', 'All E...   \n",
       "...                                                   ...   \n",
       "524812  ['Computers', 'Computers', 'Computers', 'Compu...   \n",
       "524813  ['Amazon Devices', 'Home Audio & Theater', 'Ca...   \n",
       "524814  ['Home Audio & Theater', 'Computers', 'Compute...   \n",
       "524816  ['Home Audio & Theater', 'Computers', 'All Ele...   \n",
       "524818  ['Home Audio &amp; Theater', 'All Electronics'...   \n",
       "\n",
       "                                               reviewTime  length  \n",
       "1       [Timestamp('2014-10-29 00:00:00'), Timestamp('...       9  \n",
       "3       [Timestamp('2013-09-19 00:00:00'), Timestamp('...      11  \n",
       "4       [Timestamp('2015-01-22 00:00:00'), Timestamp('...      13  \n",
       "8       [Timestamp('2013-10-24 00:00:00'), Timestamp('...       9  \n",
       "9       [Timestamp('2015-10-15 00:00:00'), Timestamp('...      11  \n",
       "...                                                   ...     ...  \n",
       "524812  [Timestamp('2014-08-13 00:00:00'), Timestamp('...      20  \n",
       "524813  [Timestamp('2013-10-21 00:00:00'), Timestamp('...       8  \n",
       "524814  [Timestamp('2014-06-14 00:00:00'), Timestamp('...      23  \n",
       "524816  [Timestamp('2009-01-28 00:00:00'), Timestamp('...      44  \n",
       "524818  [Timestamp('2014-12-29 00:00:00'), Timestamp('...       8  \n",
       "\n",
       "[307257 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = pd.read_csv('./seq_data.csv')\n",
    "seq = seq[seq['length'] > 7]\n",
    "seq['asin'] = seq['asin'].apply(lambda x: ast.literal_eval(x))\n",
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B000P1O73A'] B008562SXS train\n",
      "['B000P1O73A', 'B008562SXS'] B00BFO14W8 train\n",
      "['B000P1O73A', 'B008562SXS', 'B00BFO14W8'] B004XVN1V2 train\n",
      "['B000P1O73A', 'B008562SXS', 'B00BFO14W8', 'B004XVN1V2'] B0016BVDIK train\n",
      "['B000P1O73A', 'B008562SXS', 'B00BFO14W8', 'B004XVN1V2', 'B0016BVDIK'] B0016BVDIK train\n",
      "['B008562SXS', 'B00BFO14W8', 'B004XVN1V2', 'B0016BVDIK', 'B0016BVDIK'] B004XJ6R0Q train\n",
      "['B00BFO14W8', 'B004XVN1V2', 'B0016BVDIK', 'B0016BVDIK', 'B004XJ6R0Q'] B000Z9R2QQ valid\n",
      "['B004XVN1V2', 'B0016BVDIK', 'B0016BVDIK', 'B004XJ6R0Q', 'B000Z9R2QQ'] B000Z9R2QQ test\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/a/65216213/15751564\n",
    "a = pd.Series(seq['asin'].iloc[0])\n",
    "b = [window.to_list() for window in a.rolling(window=6)]\n",
    "c = a.values.tolist()\n",
    "d = [i[:-1] for i in b]\n",
    "l = ['train'] * len(c)\n",
    "l[-1] = 'test'\n",
    "l[-2] = 'valid'\n",
    "for i in range(1, len(c)):\n",
    "    print(d[i], c[i], l[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3636: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 56s, sys: 3.08 s, total: 2min 59s\n",
      "Wall time: 2min 58s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>split</th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0003214FKMKJE0PCW3D</td>\n",
       "      <td>train</td>\n",
       "      <td>[B000P1O73A]</td>\n",
       "      <td>B008562SXS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0003214FKMKJE0PCW3D</td>\n",
       "      <td>train</td>\n",
       "      <td>[B000P1O73A, B008562SXS]</td>\n",
       "      <td>B00BFO14W8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0003214FKMKJE0PCW3D</td>\n",
       "      <td>train</td>\n",
       "      <td>[B000P1O73A, B008562SXS, B00BFO14W8]</td>\n",
       "      <td>B004XVN1V2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0003214FKMKJE0PCW3D</td>\n",
       "      <td>train</td>\n",
       "      <td>[B000P1O73A, B008562SXS, B00BFO14W8, B004XVN1V2]</td>\n",
       "      <td>B0016BVDIK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0003214FKMKJE0PCW3D</td>\n",
       "      <td>train</td>\n",
       "      <td>[B000P1O73A, B008562SXS, B00BFO14W8, B004XVN1V...</td>\n",
       "      <td>B0016BVDIK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009267</th>\n",
       "      <td>AZZZU3P1UQZ0C</td>\n",
       "      <td>train</td>\n",
       "      <td>[B0007NWL70, B01DMHPT3U, B007PUMCWC]</td>\n",
       "      <td>B00ESW9SZ4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009268</th>\n",
       "      <td>AZZZU3P1UQZ0C</td>\n",
       "      <td>train</td>\n",
       "      <td>[B0007NWL70, B01DMHPT3U, B007PUMCWC, B00ESW9SZ4]</td>\n",
       "      <td>B00092DZ6U</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009269</th>\n",
       "      <td>AZZZU3P1UQZ0C</td>\n",
       "      <td>train</td>\n",
       "      <td>[B0007NWL70, B01DMHPT3U, B007PUMCWC, B00ESW9SZ...</td>\n",
       "      <td>B014Q8XTZE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009270</th>\n",
       "      <td>AZZZU3P1UQZ0C</td>\n",
       "      <td>valid</td>\n",
       "      <td>[B01DMHPT3U, B007PUMCWC, B00ESW9SZ4, B00092DZ6...</td>\n",
       "      <td>B018APC50Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009271</th>\n",
       "      <td>AZZZU3P1UQZ0C</td>\n",
       "      <td>test</td>\n",
       "      <td>[B007PUMCWC, B00ESW9SZ4, B00092DZ6U, B014Q8XTZ...</td>\n",
       "      <td>B01H6G7ULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4009272 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   reviewerID  split  \\\n",
       "0        A0003214FKMKJE0PCW3D  train   \n",
       "1        A0003214FKMKJE0PCW3D  train   \n",
       "2        A0003214FKMKJE0PCW3D  train   \n",
       "3        A0003214FKMKJE0PCW3D  train   \n",
       "4        A0003214FKMKJE0PCW3D  train   \n",
       "...                       ...    ...   \n",
       "4009267         AZZZU3P1UQZ0C  train   \n",
       "4009268         AZZZU3P1UQZ0C  train   \n",
       "4009269         AZZZU3P1UQZ0C  train   \n",
       "4009270         AZZZU3P1UQZ0C  valid   \n",
       "4009271         AZZZU3P1UQZ0C   test   \n",
       "\n",
       "                                                     input      target  label  \n",
       "0                                             [B000P1O73A]  B008562SXS      1  \n",
       "1                                 [B000P1O73A, B008562SXS]  B00BFO14W8      1  \n",
       "2                     [B000P1O73A, B008562SXS, B00BFO14W8]  B004XVN1V2      1  \n",
       "3         [B000P1O73A, B008562SXS, B00BFO14W8, B004XVN1V2]  B0016BVDIK      1  \n",
       "4        [B000P1O73A, B008562SXS, B00BFO14W8, B004XVN1V...  B0016BVDIK      1  \n",
       "...                                                    ...         ...    ...  \n",
       "4009267               [B0007NWL70, B01DMHPT3U, B007PUMCWC]  B00ESW9SZ4      1  \n",
       "4009268   [B0007NWL70, B01DMHPT3U, B007PUMCWC, B00ESW9SZ4]  B00092DZ6U      1  \n",
       "4009269  [B0007NWL70, B01DMHPT3U, B007PUMCWC, B00ESW9SZ...  B014Q8XTZE      1  \n",
       "4009270  [B01DMHPT3U, B007PUMCWC, B00ESW9SZ4, B00092DZ6...  B018APC50Y      1  \n",
       "4009271  [B007PUMCWC, B00ESW9SZ4, B00092DZ6U, B014Q8XTZ...  B01H6G7ULC      1  \n",
       "\n",
       "[4009272 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def create_split(asin, window_size=6):\n",
    "    a = pd.Series(asin)\n",
    "    windows = [window.to_list() for window in a.rolling(window_size)] \n",
    "    # https://stackoverflow.com/a/65216213/15751564\n",
    "    input = [i[:-1] for i in windows]\n",
    "    labels = ['train'] * len(asin)\n",
    "    labels[-1] = 'test'\n",
    "    labels[-2] = 'valid'\n",
    "\n",
    "    return labels[1:], input[1:], asin[1:]\n",
    "\n",
    "def save_files(seq, sample_percentage=0.1):\n",
    "    test = seq[['reviewerID','asin']]\n",
    "    test[['split', 'input', 'target']] = test.apply(lambda r: create_split(r['asin']), axis=1, result_type=\"expand\")\n",
    "    test = test.drop(columns=['asin'])\n",
    "    t = test.set_index(['reviewerID']).apply(pd.Series.explode).reset_index()\n",
    "    t['label'] = 1\n",
    "    t.to_csv('./splitted.csv', sep='\\t', index=False)\n",
    "    t[t['split']=='train'].sample(frac=sample_percentage, random_state=4535).to_csv('./train_data.csv', sep='\\t', header=False, index=False)\n",
    "    t[t['split']=='valid'].sample(frac=sample_percentage, random_state=4535).to_csv('./valid_data.csv', sep='\\t', header=False, index=False)\n",
    "    t[t['split']=='test'].sample(frac=sample_percentage, random_state=4535).to_csv('./test_data.csv', sep='\\t', header=False, index=False)\n",
    "    return t\n",
    "\n",
    "save_files(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159305"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an Item Dict\n",
    "def create_item_dict(complete):\n",
    "    asin = complete['asin'].values.tolist()\n",
    "    asin = list(chain.from_iterable(asin)) # https://stackoverflow.com/a/29244327/15751564\n",
    "    asin = list(set(asin))\n",
    "    item_dict = {asin[i-1]:i for i in range(1,len(asin)+1)}\n",
    "\n",
    "    file = open(b\"item_dict.pkl\",\"wb\")\n",
    "    pickle.dump(item_dict, file)\n",
    "    file.close()\n",
    "\n",
    "    return item_dict, asin\n",
    "\n",
    "item_dict, asin = create_item_dict(seq)\n",
    "\n",
    "len(item_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.58 s, sys: 556 ms, total: 5.14 s\n",
      "Wall time: 5.14 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((678951, 5), (184355, 5), (1567025, 5))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Negative sampling\n",
    "def negative_sampling(filename, asin, num_negs=5):\n",
    "    with open(filename, \"r\") as f:\n",
    "        test_lines = f.readlines()\n",
    "    write_test = open(filename[:-4]+'_output.csv', \"w\")\n",
    "    for line in test_lines:\n",
    "        write_test.write(line)\n",
    "        words = line.strip().split('\\t')\n",
    "        positive_item = words[-2]\n",
    "        count = 0\n",
    "        neg_items = set()\n",
    "        while count < num_negs:\n",
    "            neg_item = random.choice(asin)\n",
    "            if neg_item == positive_item or neg_item in neg_items:\n",
    "                continue\n",
    "            count += 1\n",
    "            neg_items.add(neg_item)\n",
    "            words[-1] = \"0\"\n",
    "            words[-2] = neg_item\n",
    "            a = \"\\t\".join(words) + \"\\n\"\n",
    "            write_test.write(\"\\t\".join(words) + \"\\n\")\n",
    "\n",
    "negative_sampling('./train_data.csv', asin, num_negs=1)\n",
    "negative_sampling('./valid_data.csv', asin)\n",
    "negative_sampling('./test_data.csv', asin, num_negs=50)\n",
    "\n",
    "train = pd.read_csv('./train_data_output.csv', sep='\\t')\n",
    "valid = pd.read_csv('./valid_data_output.csv', sep='\\t')\n",
    "test = pd.read_csv('./test_data_output.csv', sep='\\t')\n",
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.72 s, sys: 244 ms, total: 8.97 s\n",
      "Wall time: 8.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file, item_dict, pad=5):\n",
    "        self.file = pd.read_csv(file, sep='\\t', names=[1,2,3,4,5])\n",
    "        self.file[3] = self.file[3].apply(lambda x: ast.literal_eval(x))\n",
    "        self.item_dict = item_dict\n",
    "        self.input = self.file[3].values\n",
    "        self.target = self.file[4].values\n",
    "        self.labels = self.file[5].values\n",
    "        self.pad = pad\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.input[idx]\n",
    "        input = [self.item_dict[i] for i in input]\n",
    "        target = self.item_dict[self.target[idx]]\n",
    "        label = [self.labels[idx]]\n",
    "        input = torch.tensor(input)\n",
    "        label = torch.tensor(label)\n",
    "        input = nn.ConstantPad1d((self.pad-input.shape[0], 0), 0)(input)\n",
    "        e_i = torch.zeros(len(self.item_dict) + 1)\n",
    "        e_i[target] = 1\n",
    "\n",
    "        return (input,e_i), label\n",
    "\n",
    "training = CustomDataset('./train_data_output.csv', item_dict)\n",
    "validation = CustomDataset('./valid_data_output.csv', item_dict)\n",
    "train_dataloader = DataLoader(training, batch_size=64, shuffle=True)\n",
    "valid_dataloader = DataLoader(validation, batch_size=60, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seqrecModel(nn.Module):\n",
    "    def __init__(self, cell, len_item, embedding_dim, hidden_dim, final_dim, seq_length,num_layers, bidirectional=False):\n",
    "        super(seqrecModel, self).__init__()\n",
    "        self.len_item = len_item + 1\n",
    "        self.embs = nn.Embedding(self.len_item, embedding_dim, padding_idx=0)\n",
    "        self.rnn = cell(embedding_dim, hidden_dim, num_layers, bidirectional=bidirectional,batch_first=True)\n",
    "        if bidirectional:\n",
    "            self.l = nn.Linear(2*hidden_dim, final_dim)\n",
    "        else:\n",
    "            self.l = nn.Linear(hidden_dim, final_dim)\n",
    "        self.l1 = nn.Linear(final_dim*seq_length+self.len_item, hidden_dim-final_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim-final_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        nn.init.xavier_uniform_(self.l.weight)\n",
    "        nn.init.zeros_(self.l.bias)\n",
    "        nn.init.xavier_uniform_(self.l1.weight)\n",
    "        nn.init.zeros_(self.l1.bias)\n",
    "        nn.init.xavier_uniform_(self.l2.weight)\n",
    "        nn.init.zeros_(self.l2.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embeds = self.embs(input[0])\n",
    "        rnn_out, _ = self.rnn(embeds)\n",
    "        l = self.l(rnn_out)\n",
    "        inp = torch.cat((l.flatten(1),input[1]), 1)\n",
    "        l1 = self.relu(self.l1(inp))\n",
    "        score = self.sigmoid(self.l2(l1))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    \"\"\"Computing ndcg score metric at k.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground-truth labels.\n",
    "        y_score (np.ndarray): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: ndcg scores.\n",
    "    \"\"\"\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    ans = actual / best\n",
    "    if best == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return actual / best\n",
    "\n",
    "def hit_score(y_true, y_score, k=10):\n",
    "    \"\"\"Computing hit score metric at k.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): ground-truth labels.\n",
    "        y_score (np.ndarray): predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: hit score.\n",
    "    \"\"\"\n",
    "    ground_truth = np.where(y_true == 1)[0]\n",
    "    argsort = np.argsort(y_score)[::-1][:k]\n",
    "    for idx in argsort:\n",
    "        if idx in ground_truth:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    \"\"\"Computing dcg score metric at k.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground-truth labels.\n",
    "        y_score (np.ndarray): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: dcg scores.\n",
    "    \"\"\"\n",
    "    k = min(np.shape(y_true)[-1], k)\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def evaluate_metrics(metric, labels, preds):\n",
    "    res = {}\n",
    "    for i in metric:\n",
    "        if i.startswith(\"ndcg\"):  # format like:  ndcg@2;4;6;8\n",
    "            ndcg_list = [1, 2]\n",
    "            ks = i.split(\"@\")\n",
    "            if len(ks) > 1:\n",
    "                ndcg_list = [int(token) for token in ks[1].split(\";\")]\n",
    "            for k in ndcg_list:\n",
    "                ndcg_temp = np.mean(\n",
    "                    [\n",
    "                        ndcg_score(each_labels, each_preds, k)\n",
    "                        for each_labels, each_preds in zip(labels, preds)\n",
    "                    ]\n",
    "                )\n",
    "                res[\"ndcg@{0}\".format(k)] = round(ndcg_temp, 4)\n",
    "                \n",
    "        elif i.startswith(\"hit\"):  # format like:  hit@2;4;6;8\n",
    "            hit_list = [1, 2]\n",
    "            ks = i.split(\"@\")\n",
    "            if len(ks) > 1:\n",
    "                hit_list = [int(token) for token in ks[1].split(\";\")]\n",
    "            for k in hit_list:\n",
    "                hit_temp = np.mean(\n",
    "                    [\n",
    "                        hit_score(each_labels, each_preds, k)\n",
    "                        for each_labels, each_preds in zip(labels, preds)\n",
    "                    ]\n",
    "                )\n",
    "                res[\"hit@{0}\".format(k)] = round(hit_temp, 4)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Test Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train(dataloader, model, loss_fn, optimizer, DEVICE):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (input, label) in enumerate(dataloader):\n",
    "        model.zero_grad()\n",
    "        input[0], input[1] = input[0].to(DEVICE), input[1].to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        preds = model(input)\n",
    "        \n",
    "        label = label.float()\n",
    "        loss = loss_fn(preds,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), batch*len(input[0])\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, DEVICE, num_negs=5):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    group = num_negs + 1\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input, label in dataloader:\n",
    "            input[0], input[1] = input[0].to(DEVICE), input[1].to(DEVICE)\n",
    "            pred = model(input)\n",
    "            pred = pred.cpu().numpy()\n",
    "            label = label.numpy()\n",
    "            \n",
    "            predictions.extend(np.reshape(pred, (-1, group)))\n",
    "            labels.extend(np.reshape(label, (-1, group)))\n",
    "            \n",
    "\n",
    "    res = evaluate_metrics(['ndcg@10', 'hit@10'], labels, predictions)\n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.690807  [    0/678952]\n",
      "loss: 0.135633  [64000/678952]\n",
      "loss: 0.169644  [128000/678952]\n",
      "loss: 0.251018  [192000/678952]\n",
      "loss: 0.125674  [256000/678952]\n",
      "loss: 0.119854  [320000/678952]\n",
      "loss: 0.187529  [384000/678952]\n",
      "loss: 0.172008  [448000/678952]\n",
      "loss: 0.120195  [512000/678952]\n",
      "loss: 0.074819  [576000/678952]\n",
      "loss: 0.051090  [640000/678952]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48683/3288085341.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ans = actual / best\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ndcg@10': 0.4384, 'hit@10': 0.6628}\n",
      "Done in 6.59540895 mins\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.115195  [    0/678952]\n",
      "loss: 0.161450  [64000/678952]\n",
      "loss: 0.111647  [128000/678952]\n",
      "loss: 0.314923  [192000/678952]\n",
      "loss: 0.078330  [256000/678952]\n",
      "loss: 0.188367  [320000/678952]\n",
      "loss: 0.191598  [384000/678952]\n",
      "loss: 0.124986  [448000/678952]\n",
      "loss: 0.114240  [512000/678952]\n",
      "loss: 0.208225  [576000/678952]\n",
      "loss: 0.069257  [640000/678952]\n",
      "{'ndcg@10': 0.4382, 'hit@10': 0.6702}\n",
      "Done in 6.461520916666667 mins\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.120227  [    0/678952]\n",
      "loss: 0.238389  [64000/678952]\n",
      "loss: 0.083099  [128000/678952]\n",
      "loss: 0.100993  [192000/678952]\n",
      "loss: 0.109095  [256000/678952]\n",
      "loss: 0.048874  [320000/678952]\n",
      "loss: 0.081494  [384000/678952]\n",
      "loss: 0.108552  [448000/678952]\n",
      "loss: 0.124853  [512000/678952]\n",
      "loss: 0.101285  [576000/678952]\n",
      "loss: 0.034829  [640000/678952]\n",
      "{'ndcg@10': 0.4402, 'hit@10': 0.6657}\n",
      "Done in 6.429085116666667 mins\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.161641  [    0/678952]\n",
      "loss: 0.155477  [64000/678952]\n",
      "loss: 0.098721  [128000/678952]\n",
      "loss: 0.096125  [192000/678952]\n",
      "loss: 51.562500  [256000/678952]\n",
      "loss: 46.875000  [320000/678952]\n",
      "loss: 40.625000  [384000/678952]\n",
      "loss: 48.437500  [448000/678952]\n",
      "loss: 40.625000  [512000/678952]\n",
      "loss: 65.625000  [576000/678952]\n",
      "loss: 37.500000  [640000/678952]\n",
      "{'ndcg@10': 0.4054, 'hit@10': 0.6632}\n",
      "Done in 6.567711416666667 mins\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 45.312500  [    0/678952]\n",
      "loss: 48.437500  [64000/678952]\n",
      "loss: 51.562500  [128000/678952]\n",
      "loss: 48.437500  [192000/678952]\n",
      "loss: 42.187500  [256000/678952]\n",
      "loss: 64.062500  [320000/678952]\n",
      "loss: 45.312500  [384000/678952]\n",
      "loss: 54.687500  [448000/678952]\n",
      "loss: 46.875000  [512000/678952]\n",
      "loss: 50.000000  [576000/678952]\n",
      "loss: 64.062500  [640000/678952]\n",
      "{'ndcg@10': 0.4072, 'hit@10': 0.6626}\n",
      "Done in 6.760324366666667 mins\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 43.750000  [    0/678952]\n",
      "loss: 56.250000  [64000/678952]\n",
      "loss: 42.187500  [128000/678952]\n",
      "loss: 53.125000  [192000/678952]\n",
      "loss: 37.500000  [256000/678952]\n",
      "loss: 51.562500  [320000/678952]\n",
      "loss: 45.312500  [384000/678952]\n",
      "loss: 43.750000  [448000/678952]\n",
      "loss: 37.500000  [512000/678952]\n",
      "loss: 56.250000  [576000/678952]\n",
      "loss: 57.812500  [640000/678952]\n",
      "{'ndcg@10': 0.4076, 'hit@10': 0.6657}\n",
      "Done in 6.6719786333333335 mins\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 59.375000  [    0/678952]\n",
      "loss: 60.937500  [64000/678952]\n",
      "loss: 40.625000  [128000/678952]\n",
      "loss: 45.312500  [192000/678952]\n",
      "loss: 39.062500  [256000/678952]\n",
      "loss: 54.687500  [320000/678952]\n",
      "loss: 60.937500  [384000/678952]\n",
      "loss: 67.187500  [448000/678952]\n",
      "loss: 45.312500  [512000/678952]\n",
      "loss: 51.562500  [576000/678952]\n",
      "loss: 50.000000  [640000/678952]\n",
      "{'ndcg@10': 0.4071, 'hit@10': 0.6683}\n",
      "Done in 6.5546069 mins\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 50.000000  [    0/678952]\n",
      "loss: 50.000000  [64000/678952]\n",
      "loss: 45.312500  [128000/678952]\n",
      "loss: 62.500000  [192000/678952]\n",
      "loss: 46.875000  [256000/678952]\n",
      "loss: 51.562500  [320000/678952]\n",
      "loss: 51.562500  [384000/678952]\n",
      "loss: 45.312500  [448000/678952]\n",
      "loss: 48.437500  [512000/678952]\n",
      "loss: 51.562500  [576000/678952]\n",
      "loss: 42.187500  [640000/678952]\n",
      "{'ndcg@10': 0.4062, 'hit@10': 0.664}\n",
      "Done in 6.3973997166666665 mins\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 40.625000  [    0/678952]\n",
      "loss: 48.437500  [64000/678952]\n",
      "loss: 43.750000  [128000/678952]\n",
      "loss: 35.937500  [192000/678952]\n",
      "loss: 45.312500  [256000/678952]\n",
      "loss: 60.937500  [320000/678952]\n",
      "loss: 43.750000  [384000/678952]\n",
      "loss: 50.000000  [448000/678952]\n",
      "loss: 54.687500  [512000/678952]\n",
      "loss: 48.437500  [576000/678952]\n",
      "loss: 46.875000  [640000/678952]\n",
      "{'ndcg@10': 0.4057, 'hit@10': 0.6658}\n",
      "Done in 6.383242083333333 mins\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 43.750000  [    0/678952]\n",
      "loss: 43.750000  [64000/678952]\n",
      "loss: 51.562500  [128000/678952]\n",
      "loss: 60.937500  [192000/678952]\n",
      "loss: 53.125000  [256000/678952]\n",
      "loss: 46.875000  [320000/678952]\n",
      "loss: 51.562500  [384000/678952]\n",
      "loss: 60.937500  [448000/678952]\n",
      "loss: 51.562500  [512000/678952]\n",
      "loss: 42.187500  [576000/678952]\n",
      "loss: 48.437500  [640000/678952]\n",
      "{'ndcg@10': 0.4047, 'hit@10': 0.6646}\n",
      "Done in 6.773958383333333 mins\n",
      "seqrecModel(\n",
      "  (embs): Embedding(159306, 100, padding_idx=0)\n",
      "  (rnn): RNN(100, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (l): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (l1): Linear(in_features=159946, out_features=128, bias=True)\n",
      "  (l2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "{'ndcg@10': 0.0625, 'hit@10': 0.1776}\n"
     ]
    }
   ],
   "source": [
    "def trainandSaving(epochs, model, lossfn, optim, PATH):\n",
    "    for t in range(epochs):\n",
    "        start = datetime.now()\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer, DEVICE)\n",
    "        results = test(valid_dataloader, model, DEVICE)\n",
    "        diff = (datetime.now() - start).total_seconds() / 60.0\n",
    "        print(f\"Done in {diff} mins\")\n",
    "    \n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "        \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for cell in [(nn.RNN,'RNN'), (nn.LSTM,'LSTM'), (nn.GRU,'GRU')]:\n",
    "    for bidirection in [True, False]:\n",
    "        model = seqrecModel(cell[0],len(item_dict), 100, 256, 128, seq_length=5,num_layers=2,bidirectional=bidirection)\n",
    "        model = model.to(DEVICE)\n",
    "        \n",
    "        loss_fn = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        trainandSaving(10, model, loss_fn, optimizer, f\"./trained/{cell[1]}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the trained models for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : RNN_True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33556/3288085341.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ans = actual / best\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ndcg@10': 0.0631, 'hit@10': 0.1792}\n",
      "-------------------\n",
      "Model : RNN_False\n",
      "{'ndcg@10': 0.0641, 'hit@10': 0.1807}\n",
      "-------------------\n",
      "Model : LSTM_True\n",
      "{'ndcg@10': 0.0636, 'hit@10': 0.1796}\n",
      "-------------------\n",
      "Model : LSTM_False\n",
      "{'ndcg@10': 0.0636, 'hit@10': 0.1801}\n",
      "-------------------\n",
      "Model : GRU_True\n",
      "{'ndcg@10': 0.0643, 'hit@10': 0.1819}\n",
      "-------------------\n",
      "Model : GRU_False\n",
      "{'ndcg@10': 0.0647, 'hit@10': 0.1825}\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "test_data = CustomDataset('./test_data_output.csv', item_dict)\n",
    "test_dataloader = DataLoader(test_data, batch_size=102, shuffle=True)\n",
    "\n",
    "RESULTS = []\n",
    "for cell in [(nn.RNN,'RNN'), (nn.LSTM,'LSTM'), (nn.GRU,'GRU')]:\n",
    "    for bidirection in [True, False]:\n",
    "        device = torch.device(\"cuda\")\n",
    "        model = seqrecModel(cell[0],len(item_dict), 100, 256, 128, seq_length=5,num_layers=2,bidirectional=bidirection)\n",
    "        model.load_state_dict(torch.load(f\"./trained/{cell[1]}_{str(bidirection)}.pt\", map_location=\"cuda:0\")) \n",
    "        model = model.to(device)\n",
    "        \n",
    "        print(f\"Model : {cell[1]}_{str(bidirection)}\")\n",
    "        res = test(test_dataloader, model, device, num_negs=50)\n",
    "        print('-------------------')\n",
    "        RESULTS.append((f\"Model_{cell[1]}_{str(bidirection)}\", res))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN = {'ndcg@10': 0.0641, 'hit@10': 0.1807}\n",
    "# RNN Biderectional = {'ndcg@10': 0.0631, 'hit@10': 0.1792}\n",
    "# LSTM = {'ndcg@10': 0.0636, 'hit@10': 0.1801}\n",
    "# LSTM Biderctional = {'ndcg@10': 0.0636, 'hit@10': 0.1796}\n",
    "# GRU Biderctional = {'ndcg@10': 0.0643, 'hit@10': 0.1819}\n",
    "# GRU = {'ndcg@10': 0.0647, 'hit@10': 0.1825}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa5e4a11af8756ad93c7b6f97699264afbc7bc8114d35f0a2ed514ffff5af831"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
